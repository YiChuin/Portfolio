{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## 系統和路徑地確認，以及前置設定"],"metadata":{"id":"Zs11Aqjmx2WJ"}},{"cell_type":"markdown","source":["**確認python版本以及tensorflow版本**"],"metadata":{"id":"c1X3aNl9wtRk"}},{"cell_type":"code","source":["\n","!pip show tensorflow\n","\n","import sys\n","print(sys.version)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"301lahr4kOqj","executionInfo":{"status":"ok","timestamp":1714392572480,"user_tz":-480,"elapsed":4956,"user":{"displayName":"evan","userId":"04426431594165525417"}},"outputId":"4322f15a-6157-4987-9bcb-36ed851ef324"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Name: tensorflow\n","Version: 2.15.0\n","Summary: TensorFlow is an open source machine learning framework for everyone.\n","Home-page: https://www.tensorflow.org/\n","Author: Google Inc.\n","Author-email: packages@tensorflow.org\n","License: Apache 2.0\n","Location: /usr/local/lib/python3.10/dist-packages\n","Requires: absl-py, astunparse, flatbuffers, gast, google-pasta, grpcio, h5py, keras, libclang, ml-dtypes, numpy, opt-einsum, packaging, protobuf, setuptools, six, tensorboard, tensorflow-estimator, tensorflow-io-gcs-filesystem, termcolor, typing-extensions, wrapt\n","Required-by: dopamine-rl, tf_keras\n","3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0]\n","Python 3.10.12\n"]}]},{"cell_type":"markdown","source":["請先確認Jupyter筆記本設置是否正確，首先點選主選單的「修改」─「筆記本設置」─「運行類別」，選擇「Python3」，同時將「硬件加速器」下拉式選單由「None」改成「GPU」，再按「保存」。接著可選擇性執行下列指令確認Colaboratory提供的虛擬機的CPU, 磁碟空間、記憶體大小及GPU是否正確啟動。若出現'/device:GPU:0'表示GPU成功啟動。\n"],"metadata":{"id":"uHtgjiftEO76"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":692},"id":"02LrENJ-EAtw","executionInfo":{"status":"ok","timestamp":1714391014351,"user_tz":-480,"elapsed":4984,"user":{"displayName":"evan","userId":"04426431594165525417"}},"outputId":"183c64ac-d25b-4d31-ced8-1c68852d56ed"},"outputs":[{"output_type":"stream","name":"stdout","text":["CPU Status:\n","model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n","model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n","model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n","model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n","model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n","model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n","model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n","model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n","model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n","model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n","model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n","model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n","model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n","model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n","model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n","model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n","\n","Disk Status:\n","Filesystem      Size  Used Avail Use% Mounted on\n","overlay         202G   31G  171G  16% /\n","tmpfs            64M     0   64M   0% /dev\n","shm              31G     0   31G   0% /dev/shm\n","/dev/root       2.0G  1.1G  849M  57% /usr/sbin/docker-init\n","tmpfs            32G   68K   32G   1% /var/colab\n","/dev/nvme0n1p1  242G   79G  164G  33% /kaggle/input\n","tmpfs            32G     0   32G   0% /proc/acpi\n","tmpfs            32G     0   32G   0% /proc/scsi\n","tmpfs            32G     0   32G   0% /sys/firmware\n","\n","RAM Status:\n","               total        used        free      shared  buff/cache   available\n","Mem:            62Gi       1.1Gi        57Gi       1.0Mi       4.0Gi        61Gi\n","Swap:             0B          0B          0B\n","\n","GPU Status:\n"]},{"output_type":"execute_result","data":{"text/plain":["'/device:GPU:0'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":2}],"source":["print(\"CPU Status:\")\n","!cat /proc/cpuinfo | grep model\\ name # 檢查CPU資訊\n","\n","print(\"\\nDisk Status:\")\n","!df -lh # 檢查磁碟空間\n","\n","print(\"\\nRAM Status:\")\n","!free -h #檢查記憶體大小\n","\n","print(\"\\nGPU Status:\")\n","import tensorflow as tf\n","tf.test.gpu_device_name() #檢查GPU是否啟動，若無啟動則會自動改由CPU執行"]},{"cell_type":"markdown","source":["**載入Google Drive中的路徑**"],"metadata":{"id":"O75zDRZyxvbI"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"Tl0EhRYqxp6n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 函數定義"],"metadata":{"id":"mp_04Nl6yBnB"}},{"cell_type":"markdown","source":["設定卷積神經網絡（CNN）中的操作"],"metadata":{"id":"iGV_gVeFw6VD"}},{"cell_type":"code","source":["import numpy as np\n","from numba import cuda\n","import math\n","\n","\n","def expand_data(data):\n","    '''\n","    這個函數的目的是將給定的三維數據張量（形狀為 ((x,y,z)）擴展為更大的尺寸（形狀為(x∗3,y∗3,z)）。\n","    這是為了準備將輸入數據與卷積核進行卷積操作，因為這裡使用了 3×3 的卷積核並且步長為 3。\n","    該函數會將原始數據在每個維度上擴展三倍。\n","    '''\n","\n","    d1 = np.zeros([data.shape[0]*3, data.shape[1], data.shape[2]])\n","\n","    for i in range(data.shape[0]):\n","        if i >= (data.shape[0] - 2):\n","            d1[i*3:(i*3+data.shape[0]-i),:,:] = data[i:,:,:]\n","        else:\n","            d1[i*3:(i*3)+3,:,:] = data[i:i+3,:,:]\n","\n","\n","    d2 = np.zeros([d1.shape[0], d1.shape[1]*3, d1.shape[2]])\n","\n","    for j in range(d1.shape[1]):\n","        if j >= (d1.shape[1] - 2):\n","            d2[:,j*3:(j*3+d1.shape[1]-j),:] = d1[:,j:,:]\n","        else:\n","            d2[:,j*3:(j*3)+3,:] = d1[:,j:(j+3),:]\n","    return d2\n","\n","\n","def preprocess_kernel(data):\n","\n","\n","    '''\n","    這個函數對擴展後的數據進行預處理，以準備卷積操作。\n","    它首先創建了兩個和輸入數據相同形狀的全零數據，然後對於每個 3×3 的子區塊，根據指定的公式進行處理。\n","    這個公式可以理解為對子區塊應用了某種卷積核，將其乘以不同的權重，然後對所有子區塊的處理結果進行累加，\n","    最終得到預處理後的數據。\n","    '''\n","\n","    data1 = np.zeros(data.shape)\n","    data2 = np.zeros(data.shape)\n","\n","    for i in range(int(data.shape[0]/3)):\n","        k = data[(i*3):(i*3+3),:,:]\n","        data1[i*3,:,:] = 2*k[0,:,:] - k[1,:,:] - k[2,:,:]\n","        data1[i*3+1,:,:] = 2*k[1,:,:] - k[0,:,:] - k[2,:,:]\n","        data1[i*3+2,:,:] = 2*k[2,:,:] - k[0,:,:] - k[1,:,:]\n","\n","    for i in range(int(data.shape[1]/3)):\n","        k = data[:,(i*3):(i*3+3),:]\n","        data1[:,i*3,:] = 2*k[:,0,:] - k[:,1,:] - k[:,2,:]\n","        data1[:,i*3+1,:] = 2*k[:,1,:] - k[:,0,:] - k[:,2,:]\n","        data1[:,i*3+2,:] = 2*k[:,2,:] - k[:,0,:] - k[:,1,:]\n","\n","    return data1 + data2\n","\n","\n","\n","\n","\n","def self_define_cnn_kernel_process(data):\n","\n","    '''\n","    這是整個操作的主函數。它接受一個四維的輸入數據張量 (通常是作為CNN的一層輸入)，並對每個通道 (或稱為深度) 進行處理。\n","    對於每個通道，它首先調用 expand_data 對數據進行擴展，然後再調用 preprocess_kernel 進行預處理。\n","    最後返回處理後的數據張量\n","    '''\n","\n","\n","\n","    '''\n","    1. expand data from (x, y, z) to (x*3, y*3, z) (Because Conv2D convolution with stride (3,3) for our preprocess)\n","\n","    2. 3*3 kernel process:\n","\n","        [2*V_1 - V_2 - V3\n","         2*V_2 - V_1 - V3\n","         2*V_3 - V_1 - V2]\n","\n","        +\n","\n","        [2*Vt_1 - Vt_2 - Vt_3, 2*Vt_2 - Vt_1 - Vt_3, 2*Vt_3 - Vt_1 - Vt_2]\n","\n","    '''\n","    #input\n","    data_final = np.zeros([data.shape[0], data.shape[1]*3, data.shape[2]*3, data.shape[3]])\n","    for i in range(data.shape[0]):\n","        d1 = data[i,:,:,:]\n","        d1_expand = expand_data(d1)\n","        d1_final = preprocess_kernel(d1_expand)\n","        data_final[i,:,:,:] = d1_final\n","    print(data_final.shape)\n","    return data_final"],"metadata":{"id":"PpikHAawEZUP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Wide_and_Deep_CNN_model**\n","\n","和原論文相比修改了loss function"],"metadata":{"id":"z7UsaL_9EdwU"}},{"cell_type":"code","source":["import tensorflow as tf\n","from keras import backend as K\n","# from tensorflow.keras.utils import np_utils\n","from keras.layers import Input, concatenate ,Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n","from keras.models import Model\n","\n","\n","# Focal Loss\n","def focal_loss(gamma=2., alpha=0.25):\n","    def focal_loss_fixed(y_true, y_pred):\n","        pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n","        pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n","        return -K.mean(alpha * K.pow(1. - pt_1, gamma) * K.log(K.epsilon()+pt_1)) - K.mean((1 - alpha) * K.pow(pt_0, gamma) * K.log(1. - pt_0 + K.epsilon()))\n","    return focal_loss_fixed\n","\n","\n","\n","def Wide_CNN(weeks, days, channel, wide_len, lr=0.005, decay=1e-5,momentum=0.9):\n","    inputs_deep = Input(shape=(weeks*3, days*3, channel))\n","    inputs_wide = Input(shape=(wide_len,))\n","\n","    x_deep = Conv2D(32, (3, 3), strides=(3, 3), padding='same', kernel_initializer='he_normal')(inputs_deep)\n","    x_deep = MaxPooling2D(pool_size=(3, 3))(x_deep)\n","    x_deep = Flatten()(x_deep)\n","    x_deep = Dense(128, activation='relu')(x_deep)\n","\n","    x_wide = Dense(128, activation='relu')(inputs_wide)\n","\n","    x = concatenate([x_wide, x_deep])\n","    x = Dense(64, activation='relu')(x)\n","\n","    pred = Dense(1, activation='sigmoid')(x)\n","\n","    model = Model(inputs=[inputs_wide, inputs_deep], outputs=pred)\n","\n","    sgd = tf.keras.optimizers.legacy.SGD(lr=lr, decay=decay, momentum=momentum, nesterov=True)\n","    model.compile(optimizer=sgd, loss=focal_loss())\n","\n","    return model\n"],"metadata":{"id":"0GNeQ_tZEddY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**MAP**\n","\n","路徑修改成colab用的路徑，並且加上\n","```self.validation_data = validation_data```"],"metadata":{"id":"LMu4p9v3LLTq"}},{"cell_type":"code","source":["from tensorflow import keras\n","import numpy as np\n","import pandas as pd\n","from sklearn.metrics import average_precision_score\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import precision_score\n","import os\n","\n","class MyMetric(keras.callbacks.Callback):\n","\n","    def __init__(self, train_ratio, num, validation_data):\n","        self.train_ratio = train_ratio\n","        self.num = num\n","        self.epoch = 0\n","        self.validation_data = validation_data\n","\n","\n","    def precision_at_k(self, r, k):\n","\n","        assert k >= 1\n","        r = np.asarray(r)[:k] != 0\n","        if r.size != k:\n","            raise ValueError('Relevance score length < k')\n","        return np.mean(r)\n","\n","    def average_precision(self, r):\n","\n","        r = np.asarray(r) != 0\n","        out = [self.precision_at_k(r, k + 1) for k in range(r.size) if r[k]]\n","        if not out:\n","            return 0.\n","        return np.mean(out)\n","\n","    def mean_average_precision(self, rs):\n","\n","        return np.mean([self.average_precision(r) for r in rs])\n","\n","    def on_epoch_end(self, batch, logs={}):\n","        self.epoch += 1\n","        preds = self.model.predict(self.validation_data[0:2])[:,0]\n","        y = self.validation_data[2][:,0]\n","        auc = roc_auc_score(y, preds)\n","#         print(preds.shape, y.shape)\n","\n","        temp = pd.DataFrame({'label_0':y, 'label_1':1-y, 'preds_0':preds, 'preds_1':1-preds})\n","\n","        map1 = self.mean_average_precision([list(temp.sort_values(by='preds_0',ascending=0).label_0[:100]),\n","                                list(temp.sort_values(by='preds_1',ascending=0).label_1[:100])])\n","        map2 = self.mean_average_precision([list(temp.sort_values(by='preds_0',ascending=0).label_0[:200]),\n","                                list(temp.sort_values(by='preds_1',ascending=0).label_1[:200])])\n","\n","\n","        print('AUC:%.4f     MAP@100:%.4f      MAP@200:%.4f  \\n'%(auc, map1, map2))\n","\n","        log = 'Epoch:%2d   AUC:%.4f     MAP@100:%.4f      MAP@200:%.4f  \\n'%(self.epoch, auc, map1, map2)\n","        path = '/content/drive/MyDrive/BDA/train_ratio_%.1f_num_%d.txt'%(self.train_ratio, self.num)\n","        if os.path.exists(path):\n","            fp=open(path,\"a+\",encoding=\"utf-8\")\n","            fp.write(log + '\\n')\n","        else:\n","            fp=open(path,\"w\",encoding=\"utf-8\")\n","            fp.write(log + '\\n')\n"],"metadata":{"id":"aaHzqqxLHCtZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##建立模型"],"metadata":{"id":"JIN6F29BLNEZ"}},{"cell_type":"code","source":["# 1. Read data and label\n","print('Read data and label')\n","train_data = pd.read_csv('/content/drive/MyDrive/BDA/electricity_theft_final_train.csv')"],"metadata":{"id":"oJZlbE3T1SYj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714391046466,"user_tz":-480,"elapsed":7951,"user":{"displayName":"evan","userId":"04426431594165525417"}},"outputId":"7b74df71-f116-4c34-e218-f689b92ce282"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Read data and label\n"]}]},{"cell_type":"code","source":["from datetime import datetime\n","import time\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler #正規化\n","\n","# from function import *\n","# from keras_metric import *\n","# from wide_cnn import *\n","\n","#if __name__ == '__main__':\n","\n","with tf.device('/device:GPU:0'):\n","\n","  label = train_data['FLAG'].to_frame()\n","  data = train_data.drop(['FLAG','CONS_NO'],axis='columns')\n","  data = data.iloc[:,:1029]\n","\n","\n","  scaler = MinMaxScaler()\n","  data_slr = scaler.fit_transform(data.T)\n","  data_slr = pd.DataFrame(data_slr.T)\n","\n","\n","  # 2. Split Train dataset and Test dataset with ratio (50%, 60%, 70%, 80%)\n","  # print('Split Train dataset and Test dataset with ratio (50%, 60%, 70%, 80%)')\n","\n","  valr = 0.7\n","  print('Train split ratio:%.2f'%valr)\n","\n","  X_train_wide, X_test_wide, Y_train, Y_test = train_test_split(data_slr.values, label.values, test_size = 1 - valr, random_state = 2017)\n","\n","  # 为了符合卷积神经网络模型的输入要求，CNN输入的格式是 [batch_size, height, width, channels]。\n","  '''\n","  换成一个四维数组。\n","  第一维表示样本的数量（与 DataFrame 中的行数相同）。\n","  第二维设置为 1，表示我们为每个样本保留单个通道。\n","  第三维通过将 DataFrame 中的列数除以 7 计算得到。这假设每个样本有 7 个特征。\n","  第四维设置为 7，表示每个样本有 7 个特征。\n","  再將這四維護換位子，(0,2,3,1)的意思是，把第二維移到第四維的位子\n","  '''\n","  X_train_deep = X_train_wide.reshape(X_train_wide.shape[0],1,-1,7).transpose(0,2,3,1)\n","  X_test_deep = X_test_wide.reshape(X_test_wide.shape[0],1,-1,7).transpose(0,2,3,1)\n","\n","\n","  weeks, days, channel = X_train_deep.shape[1], X_train_deep.shape[2], 1\n","  wide_len = X_train_wide.shape[1]\n","\n","  print(X_train_wide.shape, X_train_deep.shape)\n","  print(X_test_wide.shape, X_test_deep.shape)\n","\n","\n","\n","\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U5Vd38PaHpYM","executionInfo":{"status":"ok","timestamp":1714391141208,"user_tz":-480,"elapsed":1385,"user":{"displayName":"evan","userId":"04426431594165525417"}},"outputId":"04f5727d-cbc9-4c8a-f5d2-2df13eaec147"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Train split ratio:0.70\n","(29555, 1029) (29555, 147, 7, 1)\n","(12667, 1029) (12667, 147, 7, 1)\n"]}]},{"cell_type":"markdown","source":["```  with tf.device('/device:GPU:0'): ``` 用於啟動gpu，若無gpu則砍掉"],"metadata":{"id":"Fv6VTVIgyrC3"}},{"cell_type":"code","source":["  with tf.device('/device:GPU:0'):\n","    X_train_pre = self_define_cnn_kernel_process(X_train_deep)\n","    X_test_pre = self_define_cnn_kernel_process(X_test_deep)"],"metadata":{"id":"VlB2SGM9TmGU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714391257792,"user_tz":-480,"elapsed":109715,"user":{"displayName":"evan","userId":"04426431594165525417"}},"outputId":"241a4bd5-f9e5-4c78-d2b0-3285bcd753d3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(29555, 441, 21, 1)\n","(12667, 441, 21, 1)\n"]}]},{"cell_type":"markdown","source":["模型參數可嘗試去做設定"],"metadata":{"id":"HFpj3Z3EzDbY"}},{"cell_type":"code","source":["'''\n","from sklearn.model_selection import GridSearchCV\n","# 自動條參數，電腦不燒可以試試看\n","# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV\n","with tf.device('/device:GPU:0'):\n","\n","\n","'''"],"metadata":{"id":"zqfWIHv_07VC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ZvrKnUwTbydE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with tf.device('/device:GPU:0'):\n","# each model run 10 times and get the avg metric result\n","  for i in range(1):\n","      print('Round: %d'%i)\n","      model=Wide_CNN(weeks, days, channel, wide_len)\n","\n","      if i == 0:\n","          print(model.summary())\n","\n","      model.fit([X_train_wide, X_train_pre], Y_train, batch_size=64, epochs=30, verbose=1,\n","                validation_data=([X_test_wide, X_test_pre], Y_test) , callbacks = [MyMetric(train_ratio=valr, num=i, validation_data=(X_test_wide, X_test_pre, Y_test))])\n","\n","\n","\n"],"metadata":{"id":"GqmPZPeUNwz7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714391459862,"user_tz":-480,"elapsed":202076,"user":{"displayName":"evan","userId":"04426431594165525417"}},"outputId":"dc2886ac-7df9-4989-b748-aa623c99e510"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Round: 0\n","Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," input_1 (InputLayer)        [(None, 441, 21, 1)]         0         []                            \n","                                                                                                  \n"," conv2d (Conv2D)             (None, 147, 7, 32)           320       ['input_1[0][0]']             \n","                                                                                                  \n"," max_pooling2d (MaxPooling2  (None, 49, 2, 32)            0         ['conv2d[0][0]']              \n"," D)                                                                                               \n","                                                                                                  \n"," input_2 (InputLayer)        [(None, 1029)]               0         []                            \n","                                                                                                  \n"," flatten (Flatten)           (None, 3136)                 0         ['max_pooling2d[0][0]']       \n","                                                                                                  \n"," dense_1 (Dense)             (None, 128)                  131840    ['input_2[0][0]']             \n","                                                                                                  \n"," dense (Dense)               (None, 128)                  401536    ['flatten[0][0]']             \n","                                                                                                  \n"," concatenate (Concatenate)   (None, 256)                  0         ['dense_1[0][0]',             \n","                                                                     'dense[0][0]']               \n","                                                                                                  \n"," dense_2 (Dense)             (None, 64)                   16448     ['concatenate[0][0]']         \n","                                                                                                  \n"," dense_3 (Dense)             (None, 1)                    65        ['dense_2[0][0]']             \n","                                                                                                  \n","==================================================================================================\n","Total params: 550209 (2.10 MB)\n","Trainable params: 550209 (2.10 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","__________________________________________________________________________________________________\n","None\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/optimizers/legacy/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super().__init__(name, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/30\n","  2/462 [..............................] - ETA: 1:51 - loss: 0.1767 "]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0034s vs `on_train_batch_end` time: 0.0402s). Check your callbacks.\n"]},{"output_type":"stream","name":"stdout","text":["396/396 [==============================] - 2s 6ms/step\n","AUC:0.6378     MAP@100:0.5971      MAP@200:0.5933  \n","\n","462/462 [==============================] - 15s 23ms/step - loss: 0.0325 - val_loss: 0.0280\n","Epoch 2/30\n","396/396 [==============================] - 1s 2ms/step\n","AUC:0.6928     MAP@100:0.6981      MAP@200:0.7029  \n","\n","462/462 [==============================] - 5s 10ms/step - loss: 0.0262 - val_loss: 0.0265\n","Epoch 3/30\n","396/396 [==============================] - 3s 8ms/step\n","AUC:0.7149     MAP@100:0.7597      MAP@200:0.7488  \n","\n","462/462 [==============================] - 8s 16ms/step - loss: 0.0249 - val_loss: 0.0260\n","Epoch 4/30\n","396/396 [==============================] - 1s 2ms/step\n","AUC:0.7409     MAP@100:0.7814      MAP@200:0.7795  \n","\n","462/462 [==============================] - 6s 13ms/step - loss: 0.0242 - val_loss: 0.0254\n","Epoch 5/30\n","396/396 [==============================] - 1s 2ms/step\n","AUC:0.7538     MAP@100:0.8004      MAP@200:0.7891  \n","\n","462/462 [==============================] - 5s 11ms/step - loss: 0.0237 - val_loss: 0.0254\n","Epoch 6/30\n","396/396 [==============================] - 2s 6ms/step\n","AUC:0.7580     MAP@100:0.8422      MAP@200:0.8216  \n","\n","462/462 [==============================] - 7s 15ms/step - loss: 0.0231 - val_loss: 0.0250\n","Epoch 7/30\n","396/396 [==============================] - 2s 4ms/step\n","AUC:0.7631     MAP@100:0.8928      MAP@200:0.8573  \n","\n","462/462 [==============================] - 7s 15ms/step - loss: 0.0227 - val_loss: 0.0247\n","Epoch 8/30\n","396/396 [==============================] - 1s 2ms/step\n","AUC:0.7731     MAP@100:0.8980      MAP@200:0.8705  \n","\n","462/462 [==============================] - 6s 13ms/step - loss: 0.0223 - val_loss: 0.0243\n","Epoch 9/30\n","396/396 [==============================] - 2s 4ms/step\n","AUC:0.7655     MAP@100:0.8457      MAP@200:0.8363  \n","\n","462/462 [==============================] - 6s 13ms/step - loss: 0.0218 - val_loss: 0.0247\n","Epoch 10/30\n","396/396 [==============================] - 1s 2ms/step\n","AUC:0.7766     MAP@100:0.8829      MAP@200:0.8668  \n","\n","462/462 [==============================] - 5s 11ms/step - loss: 0.0215 - val_loss: 0.0244\n","Epoch 11/30\n","396/396 [==============================] - 2s 6ms/step\n","AUC:0.7800     MAP@100:0.9386      MAP@200:0.9034  \n","\n","462/462 [==============================] - 7s 15ms/step - loss: 0.0211 - val_loss: 0.0247\n","Epoch 12/30\n","396/396 [==============================] - 2s 4ms/step\n","AUC:0.7902     MAP@100:0.9221      MAP@200:0.8978  \n","\n","462/462 [==============================] - 6s 13ms/step - loss: 0.0207 - val_loss: 0.0240\n","Epoch 13/30\n","396/396 [==============================] - 1s 2ms/step\n","AUC:0.7888     MAP@100:0.9032      MAP@200:0.8787  \n","\n","462/462 [==============================] - 6s 12ms/step - loss: 0.0203 - val_loss: 0.0238\n","Epoch 14/30\n","396/396 [==============================] - 2s 6ms/step\n","AUC:0.7854     MAP@100:0.8814      MAP@200:0.8714  \n","\n","462/462 [==============================] - 7s 15ms/step - loss: 0.0200 - val_loss: 0.0240\n","Epoch 15/30\n","396/396 [==============================] - 3s 8ms/step\n","AUC:0.7939     MAP@100:0.9445      MAP@200:0.9178  \n","\n","462/462 [==============================] - 8s 16ms/step - loss: 0.0196 - val_loss: 0.0237\n","Epoch 16/30\n","396/396 [==============================] - 1s 4ms/step\n","AUC:0.7849     MAP@100:0.9498      MAP@200:0.9174  \n","\n","462/462 [==============================] - 6s 14ms/step - loss: 0.0192 - val_loss: 0.0246\n","Epoch 17/30\n","396/396 [==============================] - 2s 4ms/step\n","AUC:0.8017     MAP@100:0.9376      MAP@200:0.9080  \n","\n","462/462 [==============================] - 7s 15ms/step - loss: 0.0189 - val_loss: 0.0235\n","Epoch 18/30\n","396/396 [==============================] - 2s 6ms/step\n","AUC:0.7986     MAP@100:0.9372      MAP@200:0.9064  \n","\n","462/462 [==============================] - 7s 15ms/step - loss: 0.0185 - val_loss: 0.0237\n","Epoch 19/30\n","396/396 [==============================] - 2s 4ms/step\n","AUC:0.8047     MAP@100:0.9107      MAP@200:0.8891  \n","\n","462/462 [==============================] - 5s 11ms/step - loss: 0.0183 - val_loss: 0.0235\n","Epoch 20/30\n","396/396 [==============================] - 1s 2ms/step\n","AUC:0.8025     MAP@100:0.9237      MAP@200:0.8974  \n","\n","462/462 [==============================] - 5s 11ms/step - loss: 0.0178 - val_loss: 0.0235\n","Epoch 21/30\n","396/396 [==============================] - 2s 4ms/step\n","AUC:0.8035     MAP@100:0.9471      MAP@200:0.9206  \n","\n","462/462 [==============================] - 8s 18ms/step - loss: 0.0174 - val_loss: 0.0246\n","Epoch 22/30\n","396/396 [==============================] - 1s 2ms/step\n","AUC:0.8073     MAP@100:0.8937      MAP@200:0.8742  \n","\n","462/462 [==============================] - 4s 9ms/step - loss: 0.0170 - val_loss: 0.0236\n","Epoch 23/30\n","396/396 [==============================] - 1s 3ms/step\n","AUC:0.8091     MAP@100:0.9282      MAP@200:0.9057  \n","\n","462/462 [==============================] - 5s 11ms/step - loss: 0.0166 - val_loss: 0.0236\n","Epoch 24/30\n","396/396 [==============================] - 2s 4ms/step\n","AUC:0.7951     MAP@100:0.9376      MAP@200:0.9151  \n","\n","462/462 [==============================] - 7s 15ms/step - loss: 0.0163 - val_loss: 0.0248\n","Epoch 25/30\n","396/396 [==============================] - 2s 4ms/step\n","AUC:0.8001     MAP@100:0.9375      MAP@200:0.9150  \n","\n","462/462 [==============================] - 6s 13ms/step - loss: 0.0160 - val_loss: 0.0241\n","Epoch 26/30\n","396/396 [==============================] - 2s 4ms/step\n","AUC:0.8077     MAP@100:0.9218      MAP@200:0.9061  \n","\n","462/462 [==============================] - 6s 13ms/step - loss: 0.0156 - val_loss: 0.0239\n","Epoch 27/30\n","396/396 [==============================] - 2s 5ms/step\n","AUC:0.8015     MAP@100:0.9489      MAP@200:0.9258  \n","\n","462/462 [==============================] - 8s 17ms/step - loss: 0.0153 - val_loss: 0.0242\n","Epoch 28/30\n","396/396 [==============================] - 3s 6ms/step\n","AUC:0.8029     MAP@100:0.9538      MAP@200:0.9297  \n","\n","462/462 [==============================] - 7s 15ms/step - loss: 0.0149 - val_loss: 0.0260\n","Epoch 29/30\n","396/396 [==============================] - 1s 2ms/step\n","AUC:0.8020     MAP@100:0.9565      MAP@200:0.9349  \n","\n","462/462 [==============================] - 6s 14ms/step - loss: 0.0146 - val_loss: 0.0249\n","Epoch 30/30\n","396/396 [==============================] - 2s 4ms/step\n","AUC:0.8016     MAP@100:0.9670      MAP@200:0.9360  \n","\n","462/462 [==============================] - 6s 13ms/step - loss: 0.0142 - val_loss: 0.0254\n"]}]}]}